#!/bin/python

import os
from datetime import datetime

from CatalisUtils.daemon import DBDaemon
from CatalisUtils.logging import Logger
from CatalisUtils.database import CatalisDB
from CatalisUtils.hardware import Led

import json
from time import sleep, strftime, gmtime
import apsw, apsw.bestpractice
import requests

# initialize apsw
apsw.bestpractice.apply((
	apsw.bestpractice.connection_busy_timeout,
	apsw.bestpractice.connection_enable_foreign_keys,
	apsw.bestpractice.connection_dqs
))


# initialize environment constants ---------------------------------------------
SYNCING_FREQ 	= os.environ.get("CATALIS_SYNCING_FREQUENCY", 			"30")
REQ_RETRY_INT 	= os.environ.get("CATALIS_SYNC_REQUEST_RETRY_INTERVAL", "30")
MAX_PAYLOAD 	= os.environ.get("CATALIS_SYNC_MAX_PAYLOAD", 			"100")


# driver------------------------------------------------------------------------
if __name__ == "__main__":
	# shared object initialization
	warning_light = Led(23)
	logger = Logger(logfile=f"/var/log/catalis/csync-{strftime('%d%m%Y-%H%M%S', gmtime())}.log")
	
	# api setup
	url = "https://enpc23shl3hkn.x.pipedream.net"
	headers = {
		"Content-Type": "application/json"
	}


	# preliminary operations ---------------------------------------------------
	def prelim() -> None:
		# add to logging preamble
		logger.print(f"> API endpoint: {url}\n")


	# main loop ----------------------------------------------------------------
	def main(daemon:DBDaemon, db:CatalisDB) -> None:
		synccount = 1

		while True:
			logger.print("---------|") # print visual separator
			logger.log(f"(sync~{synccount:03}) <1/3> Fetching latest packets from database... ", endline=False)
			
			# read latest polls
			lastsync = list(db.execute("SELECT value FROM meta WHERE type=?", ("lastsync", )))[0][0]
			data = list(db.execute("""SELECT timestamp, type, data 
							FROM packets 
							WHERE timestamp > ? 
							ORDER BY timestamp ASC 
							LIMIT ?;""", (lastsync, MAX_PAYLOAD)))

			# if any new polls...
			if data:
				logger.log(f"fetched {len(data)} entries, from {datetime.utcfromtimestamp(int(lastsync)).strftime('%T')} upwards.")
				logger.log(f"(sync~{synccount:03}) <2/3> POSTing data to cloud API... ", endline=False)

				# bundle data for upload
				bundle = [{
					"timestamp": row[0], 
					"type": row[1],
					"data": json.loads(row[2])
				} for row in data]

				# send data to api endpoint
				while True:
					try:
						response = requests.post(url=url, json=json.dumps(bundle), headers=headers)
						
						if 200 <= response.status_code <= 299: # sucessfully posted data
							logger.log(f"uploaded to cloud.")

							warning_light.off()
							break
					
						else:
							logger.cap(f"error ({response.status_code}).")
							logger.log(f"HTTP error: {response.reason}", level=Logger.TRACE)
							logger.log("Retrying POST request... ", endline=False)

					except requests.exceptions.MissingSchema as e: # fatally exit if invalid url
						logger.cap("error.")
						logger.log(("Invalid URL (missing scheme) supplied for API endpoint. "
									"Check value supplied in config."), level=Logger.FATAL)
						logger.log(e, level=Logger.TRACE)

						daemon._exit_fatal(cleanup)
						
					except requests.exceptions.ConnectionError as e:
						if "NameResolutionError" in str(e): logger.cap("error (NameResolutionError).")
						if "NewConnectionError" in str(e): logger.cap("error (NameResolutionError).")
						logger.log(f"Failed to connect to API. Trying again in {REQ_RETRY_INT}s...", level=Logger.ERROR, endline=False)

					warning_light.on()
					sleep(int(REQ_RETRY_INT))

				# set sync state to last entry's timestamp
				try:
					logger.log(f"(sync~{synccount:03}) <3/3> Updating sync state... ", endline=False)
					db.execute("UPDATE meta SET value=? WHERE type=?", (data[-1][0], "lastsync" ))
					logger.log(f"latest sync timestamp: {datetime.utcfromtimestamp(int(lastsync)).strftime('%T')}.")
				
				# on failure, throw fatal error...
				except (apsw.ReadOnlyError, apsw.CantOpenError) as e:
					daemon._exit_fatal(cleanup)

				synccount += 1

			else:
				logger.log("no new entries.")

			sleep(int(SYNCING_FREQ))

	
	# cleanup function ---------------------------------------------------------
	def cleanup():
		warning_light.off()


	# initialize and start daemon ----------------------------------------------
	daemon = DBDaemon(
		name="Cloud Sync", 
		prelim=prelim, 
		main=main, 
		cleanup=cleanup, 
		logger=logger
	)
	
	daemon.start()