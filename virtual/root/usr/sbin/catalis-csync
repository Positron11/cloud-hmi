#!/bin/python

import os
from datetime import datetime

from CatalisUtils.daemon import DBDaemon
from CatalisUtils.logging import Logger
from CatalisUtils.database import CatalisDB
from CatalisUtils.hardware import Led

import json
from time import sleep, strftime, gmtime
import apsw, apsw.bestpractice
import requests

# initialize apsw
apsw.bestpractice.apply((
	apsw.bestpractice.connection_busy_timeout,
	apsw.bestpractice.connection_enable_foreign_keys,
	apsw.bestpractice.connection_dqs
))


# initialize environment constants ---------------------------------------------
ENDPOINT 		= os.environ.get("CATALIS_CSYNC_API_ENDPOINT")

SYNCING_FREQ 	= os.environ.get("CATALIS_SYNCING_FREQUENCY", 			"30")
REQ_RETRY_INT 	= os.environ.get("CATALIS_SYNC_REQUEST_RETRY_INTERVAL", "30")
MAX_PAYLOAD 	= os.environ.get("CATALIS_SYNC_MAX_PAYLOAD", 			"100")


# driver------------------------------------------------------------------------
if __name__ == "__main__":
	# shared object initialization
	warning_light = Led(23)
	logger = Logger(logfile=f"/var/log/catalis/csync-{strftime('%d%m%Y-%H%M%S', gmtime())}.log")
	
	# api setup
	headers = {
		"Content-Type": "application/json"
	}


	# preliminary operations ---------------------------------------------------
	def prelim() -> None:
		# add to logging preamble
		logger.print(f"> API endpoint: {ENDPOINT}\n")


	# main loop ----------------------------------------------------------------
	def main(daemon:DBDaemon, db:CatalisDB) -> None:
		synccount = 1

		while True:
			logger.print("---------|") # print visual separator
			logger.log(f"(sync~{synccount:03}) <1/3> Fetching latest packets from DB... ", endline=False)
			
			# read latest polls
			lastsync = list(db.execute("SELECT value FROM meta WHERE type=?", ("lastsync", )))[0][0]
			data = list(db.execute("""SELECT timestamp, type, data 
							FROM packets 
							WHERE timestamp > ? 
							ORDER BY timestamp ASC 
							LIMIT ?;""", (lastsync, MAX_PAYLOAD)))

			if data: # if any new polls...
				logger.log(f"fetched {len(data)} entries ({datetime.utcfromtimestamp(int(lastsync)).strftime('%T')} onwards).")
				logger.log(f"(sync~{synccount:03}) <2/3> POSTing data to cloud API... ", endline=False)

				# bundle data for upload
				bundle = [{
					"timestamp": row[0], 
					"type": row[1],
					"data": json.loads(row[2])
				} for row in data]

				while True: # send data to api endpoint
					try:
						response = requests.post(url=ENDPOINT, json=bundle, headers=headers)
						
						if 200 <= response.status_code <= 299: # sucessfully posted data
							logger.log(f"status 200 (OK).")

							warning_light.off()
							break
					
						else:
							logger.cap(f"error ({response.status_code}).")
							logger.log(f"HTTP error: {response.reason}", level=Logger.TRACE)
							logger.log(f"Retrying request in {REQ_RETRY_INT}s... ", endline=False)

					except requests.exceptions.MissingSchema as e: # fatally exit if invalid url
						logger.cap("error.")
						logger.log(("Invalid URL (missing scheme) supplied for API endpoint. "
									"Check value supplied in config."), level=Logger.FATAL)
						logger.log(e, level=Logger.TRACE)

						daemon._exit_fatal(cleanup)
						
					except requests.exceptions.ConnectionError as e:
						if "NameResolutionError" in str(e): logger.cap("error (NameResolutionError).")
						if "NewConnectionError" in str(e): logger.cap("error (NameResolutionError).")
						logger.log(f"Failed connecting to API - retrying in {REQ_RETRY_INT}s...", level=Logger.ERROR, endline=False)

					warning_light.on() # if not broken out of loop yet, turn on wraning light
					sleep(int(REQ_RETRY_INT))

				try: # set sync state to last entry's timestamp
					logger.log(f"(sync~{synccount:03}) <3/3> Updating sync state... ", endline=False)
					db.execute("UPDATE meta SET value=? WHERE type=?", (data[-1][0], "lastsync" ))
					logger.log(f"last synced at: {datetime.utcfromtimestamp(int(data[-1][0])).strftime('%T')}.")
				
				# on failure, throw fatal error...
				except (apsw.ReadOnlyError, apsw.CantOpenError) as e:
					daemon._exit_fatal(cleanup)

				synccount += 1

			else:
				logger.log("no new entries.")

			sleep(int(SYNCING_FREQ))

	
	# cleanup function ---------------------------------------------------------
	def cleanup():
		warning_light.off()


	# initialize and start daemon ----------------------------------------------
	daemon = DBDaemon(
		name="Cloud Sync", 
		prelim=prelim, 
		main=main, 
		cleanup=cleanup, 
		logger=logger
	)
	
	daemon.start()